# -*- coding: utf-8 -*-
"""Bert News API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PTKVqPLRHj4olIWvr4VWD7Ul5d74yRe6
"""

import torch
from transformers import pipeline
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import torch  # For tensor computations and running BERT
import requests  # For fetching news articles via API
import pandas as pd  # For organizing and analyzing article data
from transformers import pipeline  # For summarization and sentiment analysis
from sentence_transformers import SentenceTransformer  # For BERT-based embeddings
from sklearn.cluster import KMeans  # For topic modeling using clustering


# Load models
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")
sentiment_analyzer = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Summarization Function
def summarize_article(content):
    if len(content.split()) < 50:  # If the content is short, return as is
        return content
    summary = summarizer(content, max_length=60, min_length=25, do_sample=False)
    return summary[0]['summary_text']

# Sentiment Analysis Function
def analyze_sentiment(content):
    sentiment = sentiment_analyzer(content)
    label = sentiment[0]['label']  # e.g. '3 stars'
    return int(label.split()[0])   # convert '3 stars' â†’ 3 (integer)

# Topic Modeling Function
def extract_topics(contents):
    embeddings = embedding_model.encode(contents)
    num_samples = len(contents)

    # Dynamically adjust the number of clusters
    num_clusters = min(5, num_samples)  # Choose the lesser of 5 or the number of articles

    if num_clusters <= 1:
        return {0: contents}  # Return all contents under one topic if we have only one article

    clustering_model = KMeans(n_clusters=num_clusters)
    clustering_model.fit(embeddings)
    cluster_labels = clustering_model.labels_

    topics = {i: [] for i in range(num_clusters)}
    for i, label in enumerate(cluster_labels):
        topics[label].append(contents[i])
    return topics

import requests
from datetime import datetime, timedelta

def fetch_location_news(api_key, location, days_back=30):
    BASE_URL = "https://api.thenewsapi.com/v1/news/all"
    start_date = (datetime.utcnow() - timedelta(days=days_back)).strftime("%Y-%m-%d")
    all_articles = []
    page = 1

    while True:
        params = {
            "api_token": api_key,
            "language": "en",
            "search": location,
            "limit": 100,           # max per page
            "page": page,
            "published_after": start_date
        }

        response = requests.get(BASE_URL, params=params)
        print(f"Fetching page {page}... status: {response.status_code}")
        if response.status_code != 200:
            print(f"Error: {response.text}")
            break

        data = response.json().get("data", [])
        if not data:
            print("No more articles found.")
            break

        filtered_data = [
            article for article in data
            if article.get("relevance_score") is not None and article["relevance_score"] > 5
        ]

        all_articles.extend(filtered_data)
        page += 1

    print(f"Fetched {len(all_articles)} relevant articles about '{location}' since {start_date}")
    return all_articles

api_key = "Vjt328mOBEGo2AVyLJo3FIlpvvt3FLtkI16vIj4X"
arizona_articles = fetch_location_news(api_key, location="Arizona")

# Preview the results
for article in arizona_articles[:5]:
    print(f"[{article['published_at']}] {article['title']} ({article['source']})")
    print(article['url'])
    print("-" * 60)

arizona_articles

def analyze_news(news_articles):
    results = []

    for article in news_articles:
        title = article.get("title", "No title")
        description = article.get("description", "No description")
        content = article.get("snippet", "No content")
        if not content:
            continue

        # Summarization
        summary = summarize_article(content)

        # Sentiment Analysis
        sentiment = analyze_sentiment(content)

        # Prepare content for topic modeling
        results.append({
            "title": title,
            "summary": summary,
            "sentiment": sentiment
        })

    # Topic Modeling
    contents = [article['summary'] for article in results]
    topics = extract_topics(contents)

    return results, topics


analyzed_articles, topics = analyze_news(arizona_articles)

# Display results
for article in analyzed_articles:
    print(f"Title: {article['title']}")
    print(f"Summary: {article['summary']}")
    print(f"Sentiment: {article['sentiment']}")
    print('-' * 80)

# Display topics
print("\nDetected Topics:")
for topic_id, articles in topics.items():
    print(f"\nTopic {topic_id + 1}:")
    for art in articles[:3]:  # Show first 3 articles per topic
        print(f"- {art}")

import sqlite3

def store_full_analyzed_articles(location_name, raw_articles, analyzed_articles, db_name="location_news_full.db"):
    conn = sqlite3.connect(db_name)  #  saves to your local directory
    c = conn.cursor()

    # Create table with extended fields
    c.execute('''
        CREATE TABLE IF NOT EXISTS news_analysis (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            location TEXT,
            title TEXT,
            description TEXT,
            snippet TEXT,
            url TEXT,
            image_url TEXT,
            language TEXT,
            published_at TEXT,
            source TEXT,
            categories TEXT,
            relevance_score REAL,
            summary TEXT,
            sentiment TEXT
        )
    ''')

    # Insert each article with enriched metadata
    for raw, analyzed in zip(raw_articles, analyzed_articles):
        c.execute('''
            INSERT INTO news_analysis (
                location, title, description, snippet, url, image_url, language,
                published_at, source, categories, relevance_score, summary, sentiment
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            location_name,
            raw.get("title"),
            raw.get("description"),
            raw.get("snippet"),
            raw.get("url"),
            raw.get("image_url"),
            raw.get("language"),
            raw.get("published_at"),
            raw.get("source"),
            ", ".join(raw.get("categories", [])) if raw.get("categories") else None,
            raw.get("relevance_score"),
            analyzed.get("summary"),
            analyzed.get("sentiment")
        ))

    conn.commit()
    conn.close()
    return db_name

db_file_path = store_full_analyzed_articles("Arizona", arizona_articles, analyzed_articles)
print(f"Saved to: {db_file_path}")

import pandas as pd
df = pd.read_sql("SELECT * FROM news_analysis", sqlite3.connect("location_news_full.db"))
df

# calculate state score

def compute_state_sentiment(analyzed_articles):
    scores = [article['sentiment'] for article in analyzed_articles]

    if not scores:
        return None, "No articles"

    avg = sum(scores) / len(scores)



    return round(avg, 2)

compute_state_sentiment(analyzed_articles)

# Generalize it to all the states

import pandas as pd

depo_location=pd.read_csv('Regeneron_cleaned_triallocation.csv')
depo_location

depo_location

# Get a list of unique states from the 'State' column
unique_states = depo_location['State'].dropna().unique()
unique_states.sort()
unique_states.tolist()
len(unique_states)

us_states = depo_location[depo_location['Country'] == 'United States']['State'].dropna().unique()
us_states.sort()
us_states.tolist()
len(us_states)

api_key='EpCH2HzrFgqr9fEUdYubTdYp0BIRXjE4cnHcVlnO'

# limited version

def fetch_location_news(api_key, location="Arizona", days_back=30, max_pages=2):
    BASE_URL = "https://api.thenewsapi.com/v1/news/all"
    start_date = (datetime.utcnow() - timedelta(days=days_back)).strftime("%Y-%m-%d")
    all_articles = []
    page = 1

    while page <= max_pages:
        params = {
            "api_token": api_key,
            "language": "en",
            "search": location,
            "limit": 100,
            "page": page,
            "published_after": start_date
        }

        response = requests.get(BASE_URL, params=params)
        if response.status_code != 200:
            print(f"[{location}] Page {page} - Error: {response.text}")
            break

        data = response.json().get("data", [])
        if not data:
            break

        filtered_data = [
            article for article in data
            if article.get("relevance_score") and article["relevance_score"] > 5
        ]

        all_articles.extend(filtered_data)
        page += 1

    return all_articles

def analyze_all_states(api_key, db_name="location_news_full.db"):
    sentiment_scores = {}

    for state in us_states:
        print(f"\nProcessing: {state}")
        raw_articles = fetch_location_news(api_key, state)

        if not raw_articles:
            print(f"No articles found for {state}")
            sentiment_scores[state] = None
            continue

        analyzed_articles, _ = analyze_news(raw_articles)

        # Store into SQLite
        store_full_analyzed_articles(state, raw_articles, analyzed_articles, db_name)

        # Compute sentiment
        score = compute_state_sentiment(analyzed_articles)
        sentiment_scores[state] = score
        print(f"{state} sentiment score: {score}")

    return sentiment_scores

sentiment_scores= analyze_all_states(api_key, db_name="location_news_full.db")

sentiment_scores

import csv
sentiment_data = [{"State": state, "Sentiment Score": score} for state, score in sentiment_scores.items()]
csv_filename = "/mnt/data/state_sentiment_scores.csv"

with open("state_sentiment_scores.csv", mode="w", newline="") as file:
    writer = csv.DictWriter(file, fieldnames=["State", "Sentiment Score"])
    writer.writeheader()
    writer.writerows(sentiment_data)

print("âœ… Saved to state_sentiment_scores.csv")

db_file_path = store_full_analyzed_articles("Arizona", arizona_articles, analyzed_articles)
print(f"Saved to: {db_file_path}")

import pandas as pd
df = pd.read_sql("SELECT * FROM news_analysis", sqlite3.connect("location_news_full.db"))
df

df = pd.read_csv("active_sites_filtered.csv")
countries = df["Country"].dropna().unique().tolist()

len(countries)

